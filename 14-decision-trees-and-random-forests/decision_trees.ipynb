{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Decision Trees\n",
    " \n",
    "_Author: Joseph Nelson (DC)_\n",
    "\n",
    "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "Students will be able to:\n",
    "\n",
    "- Explain how a decision tree is created.\n",
    "- Build a decision tree model in scikit-learn.\n",
    "- Tune a decision tree model and explain how tuning impacts the model.\n",
    "- Interpret a tree diagram.\n",
    "- Describe the key differences between regression and classification trees.\n",
    "- Decide whether or not a decision tree is an appropriate model for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lesson Guide\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "\n",
    "\n",
    "- [Part 1: Regression Trees](#part-one)\n",
    "    - [Group Exercise](#group-exercise)\n",
    "    - [Building a Regression Tree by Hand](#by-hand)\n",
    "    - [How Does a Computer build a Regression Tree?](#computer-build)\n",
    "    - [Demo: Choosing the Ideal Cutpoint for a Given Feature](#cutpoint-demo)\n",
    "    - [Building a Regression Tree in `scikit-learn`](#sklearn-tree)\n",
    "    - [What Happens When We Grow a Tree Too Deep?](#too-deep)\n",
    "    - [Tuning a Regression Tree](#tuning-tree)\n",
    "    - [Making Predictions for the Testing Data](#testing-preds)\n",
    "\n",
    "\n",
    "- [Part 2: Classification Trees](#part-two)\n",
    "    - [Comparing Regression Trees and Classification Trees](#comparing-trees)\n",
    "    - [Splitting Criteria for Classification Trees](#splitting-criteria)\n",
    "    - [Building a Classification Tree in `scikit-learn`](#sklearn-ctree)\n",
    "\n",
    "\n",
    "- [Summary: Comparing Decision Trees With Other Models](#part-three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction\n",
    "\n",
    "In this lesson, we will be exploring **decision trees** and **random forests**. These are non-parametric models that can either be used for regression or classification. \n",
    "\n",
    "You are likely very familiar with decision trees already! For example, take this graphic from the New York Times. It explains how to predict whether Barack Obama or Hillary Clinton will win the Democratic primary in a particular county in 2008. At the same time, it informs which features might be most important to predict an Obama or Clinton win:\n",
    "\n",
    "![Obama-Clinton decision tree](assets/obama_clinton_tree.jpg)\n",
    "\n",
    "**Random forests** are groups of decision trees created using different subsets and feature sets of the training data. Each tree \"votes\" on a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why are we learning about decision trees?\n",
    "\n",
    "- They can be applied to both regression and classification problems.\n",
    "- They are easy to explain to others (interpretable).\n",
    "- They are very popular among data scientists.\n",
    "- They are the basis for more sophisticated models.\n",
    "- They have a different way of \"thinking\" than the other models we have studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"part-one\"></a>\n",
    "# Part 1: Regression Trees\n",
    "\n",
    "In this section, we will make decision trees that predict numeric data. Instead of returning a class, we will return a single numeric value for each set of conditions.\n",
    "\n",
    "For example, the following tree predicts Major League Baseball salaries based on Years playing in the major leagues and Hits the previous year. Only three different salaries are ever predicted (the average salaries of players that meet each set of conditions): \n",
    "\n",
    "![Salary tree](assets/salary_tree.png)\n",
    "\n",
    "The salary has been divided by 1,000 and log-normalized, for example a salary of \\$166,000 refers to the number:\n",
    "\n",
    "$$5.11 = \\ln(\\$166000/1000)$$\n",
    "\n",
    "Similarly, 6.00 is \\$403,000, and 6.74 is \\$846,000. A natural log transform was made because some salaries are much higher than others, leading to a non-ideal long-tail distribution.\n",
    "\n",
    "\n",
    "Our dataset will be Major League Baseball player data from 1986â€“87:\n",
    "\n",
    "- **Years** (x-axis): Number of years playing in the major leagues.\n",
    "- **Hits** (y-axis): Number of hits in the previous year.\n",
    "- **Salary** (color): Low salary is blue/green, high salary is red/yellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Salary data](assets/salary_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"group-exercise\"></a>\n",
    "### Group Exercise\n",
    "\n",
    "- The data set above is our **training data**.\n",
    "- We want to build a model that predicts the salary of **future players** based on years and hits.\n",
    "- We are going to \"segment\" the feature space into regions, and then use the **mean salary in each region** as the predicted Salary for future players.\n",
    "- Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
    "\n",
    "#### Rules for Segmenting\n",
    "\n",
    "- You can only use **straight lines** that are drawn one at a time.\n",
    "- Your line must either be **vertical or horizontal**.\n",
    "- Your line **stops** when it hits an existing line.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution (only look AFTER the group exercise!):** [Salary regions](assets/images/salary_regions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As shown in the solution above, the regions created by a computer are:\n",
    "\n",
    "- $R_1$: Players with **less than 5 years** of experience and a mean salary of **\\$166,000**.\n",
    "- $R_2$: Players with **5 or more years** of experience and **less than 118 hits** and mean salary of **\\$403,000**.\n",
    "- $R_3$: Players with **5 or more years** of experience and **118 hits or more** and mean Salary of **\\$846,000**.\n",
    "\n",
    "**Note:** Years and hits are both integers, but the convention is to use the **midpoint** between adjacent values to label a split.\n",
    "\n",
    "These regions are used to make predictions for **out-of-sample data**. Thus, there are only three possible predictions! (Is this different from how **linear regression** makes predictions?)\n",
    "\n",
    "Below is the equivalent regression tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Salary tree](assets/salary_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first split is **years < 4.5**, thus that split goes at the top of the tree. When a splitting rule is **true**, you follow the left branch. When a splitting rule is **false**, you follow the right branch.\n",
    "\n",
    "For players in the **left branch**, the mean salary is \\$166,000, thus you label it with that value. (Salary has been divided by 1,000 and log-transformed to 5.11.)\n",
    "\n",
    "For players in the **right branch**, there is a further split on **hits < 117.5**, dividing players into two more salary regions: \\$403,000 (transformed to 6.00), and \\$846,000 (transformed to 6.74)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Salary tree annotated](assets/salary_tree_annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What does this tree tell you about your data?**\n",
    "\n",
    "- Years is the most important factor determining salary, with a lower number of years corresponding to a lower salary.\n",
    "- For a player with a lower number of years, hits is not an important factor in determining salary.\n",
    "- For a player with a higher number of years, hits is an important factor in determining salary, with a greater number of hits corresponding to a higher salary.\n",
    "\n",
    "**Question:** What do you like and dislike about decision trees so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"by-hand\"></a>\n",
    "## Building a Regression Tree by Hand\n",
    "\n",
    "Your **training data** is a tiny data set of [used vehicle sale prices](./datasets/vehicles_train.csv). Your goal is to **predict price** for testing data.\n",
    "\n",
    "1. Read the data into a Pandas DataFrame.\n",
    "2. Explore the data by sorting, plotting, or performing split-apply-combine (a.k.a. `group_by`).\n",
    "3. Decide which feature is the most important predictor, and use that to create your first splitting rule.\n",
    "    - Only binary splits are allowed.\n",
    "4. After making your first split, split your DataFrame into two parts and then explore each part to figure out what other splits to make.\n",
    "5. Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting.\n",
    "    - Your goal is to build a model that generalizes well.\n",
    "    - You are allowed to split on the same variable multiple times.\n",
    "6. Draw your tree, labeling the leaves with the mean price for the observations in that region.\n",
    "    - Make sure nothing is backwards: You follow the **left branch** if the rule is true and the **right branch** if the rule is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"computer-build\"></a>\n",
    "## How Does a Computer Build a Regression Tree?\n",
    "\n",
    "**Ideal approach:** Considering every possible partition of the feature space (computationally infeasible).\n",
    "\n",
    "**\"Good enough\" approach:** Recursive binary splitting.\n",
    "\n",
    "1. Begin at the top of the tree.\n",
    "2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint so that the resulting tree has the lowest possible mean squared error (MSE). Make that split.\n",
    "3. Examine the two resulting regions. Once again, make a **single split** (in one of the regions) to minimize the MSE.\n",
    "4. Keep repeating Step 3 until a **stopping criterion** is met:\n",
    "    - Maximum tree depth (maximum number of splits required to arrive at a leaf).\n",
    "    - Minimum number of observations in a leaf.\n",
    "\n",
    "---\n",
    "\n",
    "This is a **greedy algorithm** because it makes locally optimal decisions -- it takes the best split at each step. A greedy algorithm hopes that a series of locally optimal decisions might be optimal overall; however, this is not always the case. For example:\n",
    "\n",
    "- Always eating cookies to maximize your immediate happiness (greedy) might not lead to optimal overall happiness.\n",
    "\n",
    "- In our case, reorganizing parts of the tree already constructed based on future splits might result in a better model overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"cutpoint-demo\"></a>\n",
    "### Demo: Choosing the Ideal Cutpoint for a Given Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# allow plots to appear in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# Vehicle data\n",
    "path = './data/vehicles_train.csv'\n",
    "train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype   prediction\n",
       "0   22000  2012   13000      2    car  6571.428571\n",
       "1   14000  2010   30000      2    car  6571.428571\n",
       "2   13000  2010   73500      4    car  6571.428571\n",
       "3    9500  2009   78000      4    car  6571.428571\n",
       "4    9000  2007   47000      4    car  6571.428571\n",
       "5    4000  2006  124000      2    car  6571.428571\n",
       "6    3000  2004  177000      4    car  6571.428571\n",
       "7    2000  2004  209000      4  truck  6571.428571\n",
       "8    3000  2003  138000      2    car  6571.428571\n",
       "9    1900  2003  160000      4    car  6571.428571\n",
       "10   2500  2003  190000      2  truck  6571.428571\n",
       "11   5000  2001   62000      4    car  6571.428571\n",
       "12   1800  1999  163000      2  truck  6571.428571\n",
       "13   1300  1997  138000      4    car  6571.428571"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before splitting anything, just predict the mean of the entire data set.\n",
    "train['prediction'] = train.price.mean()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5936.9819859959835"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE for those predictions.\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that calculates the RMSE for a given split of miles.\n",
    "def mileage_split(miles):\n",
    "    lower_mileage_price = train[train.miles < miles].price.mean()\n",
    "    higher_mileage_price = train[train.miles >= miles].price.mean()\n",
    "    train['prediction'] = np.where(train.miles < miles, lower_mileage_price, higher_mileage_price)\n",
    "    return np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3984.09174254\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype    prediction\n",
       "0   22000  2012   13000      2    car  15000.000000\n",
       "1   14000  2010   30000      2    car  15000.000000\n",
       "2   13000  2010   73500      4    car   4272.727273\n",
       "3    9500  2009   78000      4    car   4272.727273\n",
       "4    9000  2007   47000      4    car  15000.000000\n",
       "5    4000  2006  124000      2    car   4272.727273\n",
       "6    3000  2004  177000      4    car   4272.727273\n",
       "7    2000  2004  209000      4  truck   4272.727273\n",
       "8    3000  2003  138000      2    car   4272.727273\n",
       "9    1900  2003  160000      4    car   4272.727273\n",
       "10   2500  2003  190000      2  truck   4272.727273\n",
       "11   5000  2001   62000      4    car   4272.727273\n",
       "12   1800  1999  163000      2  truck   4272.727273\n",
       "13   1300  1997  138000      4    car   4272.727273"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE for tree that splits on miles < 50,000.\n",
    "print 'RMSE:', mileage_split(50000)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3530.14653008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>12083.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>2437.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype    prediction\n",
       "0   22000  2012   13000      2    car  12083.333333\n",
       "1   14000  2010   30000      2    car  12083.333333\n",
       "2   13000  2010   73500      4    car  12083.333333\n",
       "3    9500  2009   78000      4    car  12083.333333\n",
       "4    9000  2007   47000      4    car  12083.333333\n",
       "5    4000  2006  124000      2    car   2437.500000\n",
       "6    3000  2004  177000      4    car   2437.500000\n",
       "7    2000  2004  209000      4  truck   2437.500000\n",
       "8    3000  2003  138000      2    car   2437.500000\n",
       "9    1900  2003  160000      4    car   2437.500000\n",
       "10   2500  2003  190000      2  truck   2437.500000\n",
       "11   5000  2001   62000      4    car  12083.333333\n",
       "12   1800  1999  163000      2  truck   2437.500000\n",
       "13   1300  1997  138000      4    car   2437.500000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE for tree that splits on miles < 100,000.\n",
    "print 'RMSE:', mileage_split(100000)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check all possible mileage splits.\n",
    "mileage_range = range(train.miles.min(), train.miles.max(), 1000)\n",
    "RMSE = [mileage_split(miles) for miles in mileage_range]\n",
    "# RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAESCAYAAAAmOQivAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f04cc57d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot mileage cutpoint (x-axis) versus RMSE (y-axis).\n",
    "plt.plot(mileage_range, RMSE);\n",
    "plt.xlabel('Mileage cutpoint');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Recap:** Before every split, we repeat this process for every feature and choose the feature and cutpoint that produce the lowest MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"sklearn-tree\"></a>\n",
    "## Building a Regression Tree in `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Encode car as 0 and truck as 1.\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "feature_cols = ['year', 'miles', 'doors', 'vtype']\n",
    "\n",
    "X = train[feature_cols]\n",
    "y = train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a DecisionTreeRegressor (with random_state=1).\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3107.1428571428573"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use leave-one-out cross-validation (LOOCV) to estimate the RMSE for this model.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(treereg, X, y, cv=14, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"too-deep\"></a>\n",
    "## What Happens When We Grow a Tree Too Deep?\n",
    "\n",
    "- **On the left:** A regression tree for salary that is **grown deeper**.\n",
    "- **On the right:** A comparison of the **training, testing, and cross-validation errors** for trees with different numbers of leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Salary tree grown deep](assets/salary_tree_deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **training error** continues to go down as the tree size increases (due to overfitting), but the lowest **cross-validation error** occurs for a tree with three leaves. \n",
    "\n",
    "Note that if we make a **complete tree** (where every data point is boxed into its own region), then we will achieve perfect training accuracy. However, then outliers in the training data will greatly affect the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"#tuning-tree\"></a>\n",
    "## Tuning a Regression Tree\n",
    "\n",
    "Let's try to reduce the RMSE by tuning the **max_depth** parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4050.1443001442999"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try different values one by one.\n",
    "treereg = DecisionTreeRegressor(max_depth=1, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=14, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or, we could write a loop to try a range of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# List of values to try:\n",
    "max_depth_range = range(1, 8)\n",
    "\n",
    "# List to store the average RMSE for each value of max_depth:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use LOOCV with each value of max_depth.\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=14, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAESCAYAAAAmOQivAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNyIfbXBMngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6f04b43450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot max_depth (x-axis) versus RMSE (y-axis).\n",
    "plt.plot(max_depth_range, RMSE_scores);\n",
    "plt.xlabel('max_depth');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter.\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.798744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miles</td>\n",
       "      <td>0.201256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vtype</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance\n",
       "0    year    0.798744\n",
       "1   miles    0.201256\n",
       "2   doors    0.000000\n",
       "3   vtype    0.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature.\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating a Tree Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To create a tree diagram, we will use the Graphviz library for displaying graph data structures. \n",
    "\n",
    "Surprisingly, every tree is just a graph in disguise! A graph is a tree only if there is exactly one vertex with no incoming edge (the root), while all other vertices have exactly one incoming edge (representing its parent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Graphviz file.\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(treereg, out_file='./assets/tree_vehicles.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_vehicles.dot -o tree_vehicles.png\n",
    "\n",
    "# Or, you can drag the image below to your desktop or Powerpoint!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Tree for vehicle data](assets/tree_vehicles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reading the internal nodes:\n",
    "\n",
    "- **samples:** Number of observations in that node before splitting.\n",
    "- **mse:** MSE calculated by comparing the actual response values in that node against the mean response value in that node.\n",
    "- **rule:** Rule used to split that node (go left if true, go right if false).\n",
    "\n",
    "Reading the leaves:\n",
    "\n",
    "- **samples:** Number of observations in that node.\n",
    "- **value:** Mean response value in that node.\n",
    "- **mse:** MSE calculated by comparing the actual response values in that node against \"value.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"testing-preds\"></a>\n",
    "## Making Predictions for the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  vtype\n",
       "0   3000  2003  130000      4      1\n",
       "1   6000  2005   82500      4      0\n",
       "2  12000  2010   60000      2      0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the testing data.\n",
    "path = './data/vehicles_test.csv'\n",
    "\n",
    "test = pd.read_csv(path)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question:** Using the tree diagram above, what predictions will the model make for each observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4000.,   5000.,  13500.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use fitted model to make predictions on testing data.\n",
    "X_test = test[feature_cols]\n",
    "y_test = test.price\n",
    "y_pred = treereg.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190.2380714238084"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7937.2539331937714"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE for your own tree.\n",
    "y_test = [3000, 6000, 12000]\n",
    "y_pred = [0, 0, 0]\n",
    "\n",
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"part-two\"></a>\n",
    "# Part 2: Classification Trees\n",
    "\n",
    "**Example:** Predict whether or not Barack Obama or Hillary Clinton will win the Democratic primary in a particular county in 2008 (from the New York Times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Obama-Clinton decision tree](assets/obama_clinton_tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Questions:**\n",
    "\n",
    "- What are the observations? How many observations are there?\n",
    "- What is the response variable?\n",
    "- What are the features?\n",
    "- What is the most predictive feature?\n",
    "- Why does the tree split on high school graduation rate twice in a row?\n",
    "- What is the class prediction for the following county: 15 percent African American, 90 percent high school graduation rate, located in the South, high poverty, high population density?\n",
    "- What is the predicted probability for that same county?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"comparing-trees\"></a>\n",
    "## Comparing Regression Trees and Classification Trees\n",
    "\n",
    "|Regression Trees|Classification Trees|\n",
    "|---|---|\n",
    "|Predict a continuous response.|Predict a categorical response.|\n",
    "|Predict using mean response of each leaf.|Predict using most commonly occurring class of each leaf.|\n",
    "|Splits are chosen to minimize MSE.|Splits are chosen to minimize Gini index (discussed below).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"splitting-criteria\"></a>\n",
    "## Splitting Criteria for Classification Trees\n",
    "\n",
    "Common options for the splitting criteria:\n",
    "\n",
    "- **Classification error rate:** The fraction of training observations in a region that don't belong to the most common class.\n",
    "- **Gini index:** The measure of total variance across classes in a region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Classification Error Rate\n",
    "\n",
    "Pretend we are predicting whether or not someone will buy an iPhone or an Android:\n",
    "\n",
    "- At a particular node, there are **25 observations** (phone buyers) of whom **10 bought iPhones and 15 bought Androids**.\n",
    "- As the majority class is **Android**, that's our prediction for all 25 observations, and thus the classification error rate is **10/25 = 40%**.\n",
    "\n",
    "Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender:\n",
    "\n",
    "- **Males:** Two iPhones and 12 Androids, thus the predicted class is Android.\n",
    "- **Females:** Eight iPhones and three Androids, thus the predicted class is iPhone.\n",
    "- Classification error rate after this split would be **5/25 = 20%**.\n",
    "\n",
    "Compare that with a split on age:\n",
    "\n",
    "- **30 or younger:** Four iPhones and eight Androids, thus the predicted class is Android.\n",
    "- **31 or older:** Six iPhones and seven Androids, thus the predicted class is Android.\n",
    "- Classification error rate after this split would be **10/25 = 40%**.\n",
    "\n",
    "The decision tree algorithm will try **every possible split across all features** and choose the one that **reduces the error rate the most.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gini Index\n",
    "\n",
    "Calculate the Gini index before making a split:\n",
    "\n",
    "$$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
    "\n",
    "- The **maximum value** of the Gini index is 0.5 and occurs when the classes are perfectly balanced in a node.\n",
    "- The **minimum value** of the Gini index is 0 and occurs when there is only one class represented in a node.\n",
    "- A node with a lower Gini index is said to be more \"pure.\"\n",
    "\n",
    "Evaluating the split on **gender** using the Gini index:\n",
    "\n",
    "$$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$\n",
    "$$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$\n",
    "$$\\text{Weighted Average: } 0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$\n",
    "\n",
    "Evaluating the split on **age** using the Gini index:\n",
    "\n",
    "$$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$\n",
    "$$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$\n",
    "$$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$\n",
    "\n",
    "Again, the decision tree algorithm will try **every possible split** and will choose the one that **reduces the Gini index (and thus increases the \"node purity\") the most**.\n",
    "\n",
    "You can think of this as each split increasing the accuracy of predictions. If there is some error at a node, then splitting at that node will result in two nodes with a higher average \"node purity\" than the original. So, we ensure continually better fits to the training data by continually splitting nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing Classification Error Rate and Gini Index\n",
    "\n",
    "- Gini index is generally preferred because it will make splits that **increase node purity**, even if that split does not change the classification error rate.\n",
    "- Node purity is important because we're interested in the **class proportions** in each region, as that's how we calculate the **predicted probability** of each class.\n",
    "- scikit-learn's default splitting criteria for classification trees is Gini index.\n",
    "\n",
    "**Note:** There is another common splitting criteria called **cross-entropy**. It's numerically similar to Gini index but slower to compute. So, it's not as popular as Gini index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"sklearn-ctree\"></a>\n",
    "## Building a Classification Tree in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll build a classification tree using the Titanic survival data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    1  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  Embarked_Q  Embarked_S  \n",
       "0         A/5 21171   7.2500   NaN        S           0           1  \n",
       "1          PC 17599  71.2833   C85        C           0           0  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S           0           1  \n",
       "3            113803  53.1000  C123        S           0           1  \n",
       "4            373450   8.0500   NaN        S           0           1  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data.\n",
    "path = './data/titanic.csv'\n",
    "titanic = pd.read_csv(path)\n",
    "\n",
    "# Encode female as 0 and male as 1.\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n",
    "\n",
    "# Fill in the missing values for age with the median age.\n",
    "titanic.Age.fillna(titanic.Age.median(), inplace=True)\n",
    "\n",
    "# Create a DataFrame of dummy variables for Embarked.\n",
    "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\n",
    "embarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the original DataFrame and the dummy DataFrame.\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# Print the updated DataFrame.\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Survived:** 0=died, 1=survived (response variable)\n",
    "- **Pclass:** 1=first class, 2=second class, 3=third class\n",
    "    - What will happen if the tree splits on this feature?\n",
    "- **Sex:** 0=female, 1=male\n",
    "- **Age:** Numeric value\n",
    "- **Embarked:** C or Q or S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
    "\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a classification tree with max_depth=3 on all data.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Graphviz file.\n",
    "export_graphviz(treeclf, out_file='./assets/tree_titanic.dot', feature_names=feature_cols)\n",
    "\n",
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_titanic.dot -o tree_titanic.png\n",
    "\n",
    "# Or, just drag this image to your desktop or Powerpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Tree for Titanic data](assets/tree_titanic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice the split in the bottom right; the **same class** is predicted in both of its leaves. That split didn't affect the **classification error rate**, although it did increase the **node purity**. This is important because it increases the accuracy of our predicted probabilities.\n",
    "\n",
    "A useful side effect of measures such as the Gini index is that they can be used give some indication of feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>0.242664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0.655584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.064494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Embarked_Q</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Embarked_S</td>\n",
       "      <td>0.037258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      Pclass    0.242664\n",
       "1         Sex    0.655584\n",
       "2         Age    0.064494\n",
       "3  Embarked_Q    0.000000\n",
       "4  Embarked_S    0.037258"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the feature importances (the Gini index at each node).\n",
    "\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"part-three\"></a>\n",
    "# Summary: Comparing Decision Trees With Other Models\n",
    "\n",
    "**Advantages of decision trees:**\n",
    "\n",
    "- They can be used for regression or classification.\n",
    "- They can be displayed graphically.\n",
    "- They are highly interpretable.\n",
    "- They can be specified as a series of rules, and more closely approximate human decision-making than other models.\n",
    "- Prediction is fast.\n",
    "- Their features don't need scaling.\n",
    "- They authomatically learn feature interactions.\n",
    "- Tends to ignore irrelevant features.\n",
    "- They are non-parametric (i.e. will outperform linear models if the relationship between features and response is highly non-linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Trees versus linear models](assets/tree_vs_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Disadvantages of decision trees:**\n",
    "\n",
    "- Their performance is (generally) not competitive with the best supervised learning methods.\n",
    "- They can easily overfit the training data (tuning is required).\n",
    "- Small variations in the data can result in a completely different tree (high variance).\n",
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree.\n",
    "- They don't tend to work well if the classes are highly unbalanced.\n",
    "- They don't tend to work well with very small data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
